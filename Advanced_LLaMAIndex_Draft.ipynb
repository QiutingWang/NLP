{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced LLaMA Index"
      ],
      "metadata": {
        "id": "g6VSinqVTs1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrievers"
      ],
      "metadata": {
        "id": "OZUBxmpsTwqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 索引方式\n",
        "- List Index:将node储存在一个序列中，node1->node2->node3,最后将所有节点放在response synthesis中，不会生成embedding所以速度很快\n",
        "- Vector Store Index：在索引过程中生成embedding存入vector中，查询该token最相似的top k的nodes\n",
        "- Tree Index：在查询时生成embedding，查询时从根节点向下遍历到叶节点，需要关注`child_branch_factor=...`的数量\n",
        "- Keyword Table Index：构建关于关键字与对应节点的映射关系\n",
        "- Summary Index: 很适合QA系统的开发，\n"
      ],
      "metadata": {
        "id": "YpEUZgyibCA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Routers\n",
        "\n",
        "- 怎么从几个候选答案中找到>1个更加合适的选择\n",
        "- 以selector modules或query engine或retriever的形式出现\n",
        "- 选择是summary index query engine文本摘要还是vector index query engine语义搜索\n",
        "- 选择是combine results together还是try out a bunch of choices at once\n",
        "  - 有的时候会使用MRR(Mean Reciprocal Rank)来判断chunk的重要性"
      ],
      "metadata": {
        "id": "vF9-syQPiqyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Selector\n",
        "-"
      ],
      "metadata": {
        "id": "0lhRRJ3iarLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb8q5eQMR54Y",
        "outputId": "87918064-d7bd-4ead-aa8f-b252576bfb59"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.8.69.post1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.23)\n",
            "Requirement already satisfied: aiostream<0.6.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.5.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.5.14)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.2.14)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
            "Requirement already satisfied: langchain>=0.0.303 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.0.335)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.2.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.5.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.0)\n",
            "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.26.18)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->llama-index) (3.20.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (6.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (4.0.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (0.0.64)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index) (0.25.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->llama-index) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->llama-index) (1.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->llama-index) (2.4)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->llama-index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai>=1.1.0->llama-index) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from llama_index import Document, VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.tools import RetrieverTool, ToolMetadata\n",
        "from llama_index.tools.query_engine import QueryEngineTool\n",
        "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.selectors.llm_selectors import LLMSingleSelector, LLMMultiSelector\n",
        "from llama_index.selectors.pydantic_selectors import PydanticSingleSelector, PydanticMultiSelector"
      ],
      "metadata": {
        "id": "OsC24jHdiwuS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSB0dK4mTfNy",
        "outputId": "ad99cdc2-1d05-4346-d434-0136a8d8b3d7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-14 05:29:07--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
            "\n",
            "\r          data/paul   0%[                    ]       0  --.-KB/s               \rdata/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-11-14 05:29:07 (2.78 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import OpenAI\n",
        "llm=OpenAI(model=\"gpt-3.5-turbo\", api_key='  ', api_base='  ')"
      ],
      "metadata": {
        "id": "6rGCsOW-np8v"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import download_loader, ServiceContext\n",
        "service_context=ServiceContext.from_defaults(llm=llm)"
      ],
      "metadata": {
        "id": "EW-GqlnLoHw8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "outputId": "0644f9ab-80ee-473f-bdee-de2beb6810ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/embeddings/utils.py\u001b[0m in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0membed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mvalidate_openai_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai_utils.py\u001b[0m in \u001b[0;36mvalidate_openai_api_key\u001b[0;34m(api_key)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopenai_api_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_API_KEY_ERROR_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-97ea4443695c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mServiceContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mservice_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mServiceContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/indices/service_context.py\u001b[0m in \u001b[0;36mfrom_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# NOTE: the embed_model isn't used in all indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0membed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve_embed_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0membed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/embeddings/utils.py\u001b[0m in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mvalidate_openai_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0;34m\"\\n******\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;34m\"Could not load OpenAI embedding model. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Router Query Engine\n",
        "- select one from multiple"
      ],
      "metadata": {
        "id": "dDQbGpKfjNHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Node PostProcessors\n",
        "\n",
        "- Retrieve后，进行transformation, 过滤，和重新排序\n",
        "- 经常和query engine在一起用"
      ],
      "metadata": {
        "id": "NoqEoExGur3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.postprocessor import SimilarityPostprocessor, CohereRerank, TimeWeightedPostprocessor\n",
        "# SimilarityPostprocessor: 将低于设置的相似度分数门槛的nodes移除\n",
        "# KeyNodePostprocessor: 确保specfic nodes包含、不包含在里面\n",
        "# CohereRerank: 重新排序，returns Top N nodes\n",
        "# TimeWeightedPostprocessor: return top K nodes with its recorded time"
      ],
      "metadata": {
        "id": "KQHde0cuwJ_k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Similaritypp=SimilarityPostprocessor(similarity_cutoff=0.9)"
      ],
      "metadata": {
        "id": "3QwkrQypw-8y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.postprocessor.node import KeywordNodePostprocessor\n",
        "KeywordNP=KeywordNodePostprocessor(required_keywords=['big'], exclude_keyword=['small'])"
      ],
      "metadata": {
        "id": "Y9XGZoeXxME6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere\n",
        "CR=CohereRerank(top_n=2,\n",
        "        model=\"rerank-english-v2.0\",\n",
        "        api_key=\"sk-aJzbu0F3j7bstWlR3e4cA9Db59Ac4f669a9f471aFa66C458\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dy8luRMxega",
        "outputId": "21cd25ea-a7ad-44fa-b099-bfe824d9db22"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-4.34-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.8.6)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere)\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.26.18)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.7.22)\n",
            "Installing collected packages: fastavro, backoff, cohere\n",
            "Successfully installed backoff-2.2.1 cohere-4.34 fastavro-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TWP=TimeWeightedPostprocessor(time_decay=0.5, top_k=3, time_access_refresh=False)"
      ],
      "metadata": {
        "id": "PCSK8cPbzKD8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import ServiceContext\n",
        "from llama_index.indices.postprocessor import EmbeddingRecencyPostprocessor\n",
        "# EmbeddingRecencyPostprocessor:按照date排序及移除较旧（时间较早）nodes和在相似度门槛以下的，后返回top k个节点\n",
        "# FixedRecencyPostprocessor: 返回按日期排序的节点\n",
        "ERP=EmbeddingRecencyPostprocessor(service_context=service_context, date_key='2023-03-01'， top_k=2)\n",
        "# service context:a bundle在indexing或querying阶段经常使用的资源，有一个设置的global configuration\n",
        "  # llm：default设置为gpt-3.5-turbo\n",
        "  # embed_model: 默认设置为BAAI/bge-small-en\n",
        "  # node_sparser: convert document into nodes\n",
        "  # prompt_helper\n",
        "from llama_index import set_global_service_context\n",
        "set_global_service_context(service_context)"
      ],
      "metadata": {
        "id": "wM-U_nnXz8a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.postprocessor import LongContextReorder\n",
        "# 当最有价值的数据放在input的开头或结尾，解决上下文扩展问题"
      ],
      "metadata": {
        "id": "IlN6zpRL3P3m"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Response Synthesizer合成器\n",
        "- a query 和对应的set of retrieved text chunks\n",
        "- 实现方式：\n",
        "  - 遍历，使用树结构\n",
        "  - 在retreiver或者node postprocessor之后"
      ],
      "metadata": {
        "id": "hLP_zQArvByW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.schema import Node, NodeWithScore\n",
        "from llama_index import get_response_synthesizer\n",
        "RS=get_response_synthesizer(structured_answer_filtering=True, response_mode=\"refine\")\n",
        "# Refine: 一个node一个node的遍历文本，精炼答案one by one。如果chunk过大，使用TokenTextSplitter\n",
        "# Compact: 将文本集中在一起提炼，需要LLM处理的步骤就减少了"
      ],
      "metadata": {
        "id": "9GUW0dZ958YY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.response_synthesizers import TreeSummarize, Refine\n",
        "# Tree Summarize: 自下向上把2个小的答案结合成一个 最后来一个总结\n",
        "summarizer=Refine(service_context=service_context, verbose=True)\n",
        "response=summarizer.get_response(\"Who is Paul Graham?\",[text])\n",
        "print(response)"
      ],
      "metadata": {
        "id": "eKr5EP1q9aXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "# 如果针对特定的范围进行嵌入，需要加上 SentenceWindowNodeParser将文档分为单独句子，并捕获周围自己的窗口\n",
        "# 将文档转化成nodes，同时保持与其他node的索引结构\n",
        "parser=SimpleNodeParser()\n",
        "nodes=parser.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "_AFZZ8f9bc5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Engine"
      ],
      "metadata": {
        "id": "TfQR0m9hfae9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=index.as_query_engine(streaming=True)\n",
        "response=query_engine.query(\"Your Question\").print_response_stream()"
      ],
      "metadata": {
        "id": "CW17sCeAidmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Engine"
      ],
      "metadata": {
        "id": "9nU8bC2ZiQ0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=index.as_chat_engine(chat_modes=...)\n",
        "# we can choose from react, openai, best, context, condense_question:从过去的检索信息中看有无可以覆盖到的, simple：直接问LLM 不会涉及到query engine\n",
        "response=query_engine.chat(\"Your Question\")"
      ],
      "metadata": {
        "id": "4CyuNNbHi7z_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}