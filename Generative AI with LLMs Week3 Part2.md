# Generative AI with LLMs Week3 Part2

- Application and Integration
  - optimize and deploy model for inference
  - build LLM-Powered Applications
- Optimization:
  - reduce the size of LLM while still maintain its performance
  - 3 Types:
    - Distillation
      - use the larger teacher model to train the smaller student model
        - The student model is trained to mimic the output distribution of the teacher model.
      - freeze the teacher model weights, generate the completion for the training data
      - generate the completion for the training data using student model
      - ğŸŒŸAchieved by *minimizing* the loss function called **Distillation Loss**
        - the difference between teacher model output <u>soft labels</u> and student model output <u>soft predictions</u> both after the *softmax layer*
        - add the `temperature parameter` to the softmax function $T$
          - affect the smoothness of output distribution
          - higher temperature, higher creativity of the model
          - $T>1$: the probability distribution becomes broader and less strongly peaked, providing tokens that are similar to *ground truth tokens*
        - train the student model with ground truth hard labels gets the output called <u>hard predictions</u>
          - Here we set $T=1$
          - the loss between the hard predictions and hard labels called **Student Loss**
      - Combine Distillation Loss & Student Loss-->*update the weights* of student model via back-propagation
      - Benefits: use only the smaller student LLM is enough.
      - Backwards: distillation does not make effect on <u>generative decoder models</u>. But it is fit for encoder-only model, such as Bert-->DistillBert, TinyBert
      - Types:
        - response/feature/relation-based knowledge
        - offline/online/self-distillation
        - KD Algorithm:
          - Multi-teacher
          - Cross-Modal
          - Data Free
          - Quantized
          - Lifelong
          - Attention-based
          - Graph-based
          - NAS-based
    - Quantization: FP32-->FP16/INT8/BFLOAT16
      - Quantization-Aware Training(QAT)åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­
        - has ability to distill large LLaMA model weights and KV cache down to 4 bits
      - Post-Training Quantization(PTQ)
        - transform model weights into a *lower precision representation*
        - æ¨¡å‹è®­ç»ƒåŠ é€Ÿæ•ˆæœå¾ˆå¥½
        - applied to <u>model weights or activation layers</u>
        - require *calibration*è¾ƒå‡†çº¿ to capture the dynamic range of original parameter values
    - Pruning:
      - remove redundant model parameters which contribute little to performance
      - é€Ÿåº¦æå‡å¹¶æ²¡æœ‰å¾ˆå¤§
      - remove model weights with valueâ‰ˆ=0
      - methods:
        - Full model re-training
        - post-training
        - PEFT
        - parameter sharing
      - Types: differences lie in pruning targets and the output network structure
        - Unstructured:
          - prunes individual parameters
          - get irregular sparse structure
          - drawbacks: result in additional LLM retraining to regain accuracy
        - Structured:
          - maintain overall network structure
          - remove connections or hierarchical sparse structure based on rules
          - advantage: reduce model complexity and memory storage required
    - Low Rank Factorization: employ matrix decomposition to find redundancy of model parameters
  - Matrices:
    - numbers of model parameter
    - model size: disk space, memory footprint
    - compression ratio:
      - the ratio between the size of original LLM & that of compressed LLM
      - higher compression ratio, more efficient compression
    - inference time
      - measures the time taken by the LLM to process and generate responses for input data
    - FLOPs: measures <u>numbers of arithmetic operations</u> involving floating-point numbers
  - Benchmark and Datasets:
    - GLUE, SuperGLUE, Big-Bench, LAMA/StrategyQA(reasoning ability of LM)
  - Materials: Large Transformer Model Inference Optimization <https://lilianweng.github.io/posts/2023-01-10-inference-optimization/>
- Some Challenges:
  - struggle with complex math
  - internal knowledge held by model cuts off during pre-trainingåªèƒ½çŸ¥é“é¢„è®­ç»ƒä¹‹å‰inputçš„çŸ¥è¯†
  - Hallucinationå¹»è§‰: the *tendency* to generate text, even when they don't know the answer
    - Type
      - ä¸Šä¸‹æ–‡
      - è¾“å…¥å†²çª
      - äº‹å®å†²çª
- Solution:
  - generally, connect the LLM to external data sources and applications
  - RAG
  - CoVe(Chain of Verification)
- LLM-Powered Applications
  - procedure: user application<--input(output)-->Orchestration library<--input(output)-->LLM
    - Orchestration library:
      - connected with external data sources with docs, databases, web, <u>vector store</u>, or existing APIs of other application
      - eg: LangChain
  - Retrieval Augmented Generation(RAG)
    - help the model understanding the updated world, reducing cost for retraining LLM with new data, more flexible
    - give the additional external data at inference time
    - increase *relevance and accuracy* of the completion
    - Components:
      - Retriever
        - <u>query encoder, external data sources</u>: find the most relevant documents(Top-K) from the data source+new information with the original user query
        - passed to LLM(Generator), producing the completion that contains the correct answer
          - Generator must be Seq2Seq model, eg: BART, T5
    - Data preparation for vector store:
      - data must with acceptable inside context window(few thousand tokens)
        - we need to split long documents into several chunks. LangChain can solve this step.
      - data must be in acceptable format(`embedding vectors`) at reference time, then stored in *vector stores* that allows fast searching of datasets
        - each text in vector store is identified by a key
        - citation is allowable
- Requirements of External Application Interaction:
  - plan actions:
    - declare what actions to take
    - instructions need to be understandable and relevant
  - Format outputs: for being understandable to broader applications
    - eg: generate python, SQL queries
  - Validate actions
    - collect required user information and make sure there are included in the completion
  - Important Materials: LLM Powered Autonomous Agents <https://lilianweng.github.io/posts/2023-06-23-agent/>
- CoT:
  - break the complex problem down into steps, think like human-->a series of intermediate reasoning steps, the chain of thought
  - pass one-shot and another question to LLM, CoT helps it generate an answer with similar structure
  - application: especially in math, coding, commonsense reasoning, symbolic reasoning
  - Advantages:
    - å›°éš¾åˆ†è§£,additional computation be allocated reasonably.
    - provide *chances to debug* by the visible reasoning window
- Program-Aided LM(PAL):
  - process:
    - Structure the example first:
      - questions, reasoning steps-->computer code, PAL execution(by an coding interpreter)
      - the text of each reasoning step begins with a pound sign as a line of comment
    - the objective new question should be shown to solve
    - pass the combined prompt to the LLM
      - generate a programming script
      - put them into a code interpreter, generate a specific answer
    - combine the answer with PAL prompt, get the PAL format solution
    - pass the updated prompt to LLM, generating the completion with correct answer
  - LLM: application reasoning engine, create the plan of action; PAL: one action to be executed; 
- ReAct:
  - <https://python.langchain.com/docs/modules/agents/agent_types/react.html>
  - a prompting strategy, called Synergizing Reasoning and Actions: CoT reasoning + action
  - reduce å¹»è§‰ and error propagationè¿ç»­çŠ¯é”™
  - procedures: 
    - few-shot instructions
    - question->thought`think[]`:identify the problems and actions we need to take->action:`åŠ¨[å®¾]ç»“æ„`ç»„æˆ(trigger the specific API actions)->observation: the result of the action
      - the allowed actions:
        - `search[entity]`: return first 5 sentences from corresponding entities
        - `finish[answer]`: finish the current task with answer
        - `lookup[keyword]`: return the next sentence in the page containing the keyword
    - repeat the cycle as many time as necessary until obtain the final answer
    - question to be answered
  - set decoding temperature $T=0.7$ during the inference in CoT
  - Evaluation metrics:
    - reasoning error: wrong reasoning trace
    - search result error: return empty or doesn't contain useful information
    - hallucination
    - label ambiguity: right prediction but without matching right label
- LangChain components:
  - prompt templates: format input examples and model completions
  - memory: use to store interactions with LLM
  - Pre-built tools
  - Agents: interpret input from users, determine which tools are used to complete the task