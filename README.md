# NLP
Natural Language Processing techniques give machines ability to ready, understand, and derive meaning from human languages.

Applications
--------------- 

Machine Translation, Speech Recognition, Sentiment Analysis, Question Answering, Automatic Summarization, Chatbots, Text Classification, Character Recognition, Spell Checking

Topic Modeling
--------------- 

- Useful Model
  - Latent Dirichlet Allocation (LDA)
     - Latent Dirichlet Allocation with Non-negative Matrix Factorization (LDA&NMF)
     - Hierarchical Latent Dirichlet Allocation (hLDA)
  - Latent Semantic Analysis (LSA)
  - Probabilistic Latent Semantic Analysis (PLSA)
  - Correlated Topic Model (CTM)
  - Author Topic Model
  - BERTopic


- Tools
  - Visualization: 
     - [pyLDAvis](https://github.com/bmabey/pyLDAvis/tree/master/notebooks)
     - [BertViz](https://github.com/jessevig/bertviz)
     - [exBert](https://huggingface.co/exbert/?model=bert-base-cased&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=0&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=true)
  - Quick LDA Topic Modeling:<https://lettier.com/projects/lda-topic-modeling/>


LLM Base-Model
---------------
- LLaMA
- GLM
- BLOOM
- GPT
- PaLM 


Paper
--------------- 
- [Latent Dirichlet Allocation](https://ai.stanford.edu/~ang/papers/jair03-lda.pdf)
- [Attention is All You Need (Transformer)](https://arxiv.org/pdf/1706.03762.pdf)
- [Algorithms for Non-negative Matrix Factorization](https://proceedings.neurips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
- [BERTopic: Neural topic modeling with a class-based TF-IDF procedure](https://arxiv.org/pdf/2203.05794.pdf)
- [T5: Exploring the Limits of Transfer Learning with a Unified
Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)
- [GPT-2: Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
- [Prompt Learning: Pre-train, Prompt, and Predict: A Systematic Survey of
Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586.pdf)
- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)
- [UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/pdf/1905.03197.pdf)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311v5.pdf)

