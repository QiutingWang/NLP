# NLP
Natural Language Processing techniques give machines ability to ready, understand, and derive meaning from human languages.

Applications
--------------- 

Machine Translation, Speech Recognition, Sentiment Analysis, Question Answering, Automatic Summarization, Chatbots, Text Classification, Character Recognition, Spell Checking

Topic Modeling
--------------- 

- Useful Model
  - Latent Dirichlet Allocation (LDA)
     - Latent Dirichlet Allocation with Non-negative Matrix Factorization (LDA&NMF)
     - Hierarchical Latent Dirichlet Allocation (hLDA)
  - Latent Semantic Analysis (LSA)
  - Probabilistic Latent Semantic Analysis (PLSA)
  - Correlated Topic Model (CTM)
  - Author Topic Model


- Tools
  - Visualization: 
     - [pyLDAvis](https://github.com/bmabey/pyLDAvis/tree/master/notebooks)
     - [BertViz](https://github.com/jessevig/bertviz)
     - [exBert](https://huggingface.co/exbert/?model=bert-base-cased&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=0&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=true)
  - Quick LDA Topic Modeling:<https://lettier.com/projects/lda-topic-modeling/>

Paper
--------------- 
- [Latent Dirichlet Allocation](https://ai.stanford.edu/~ang/papers/jair03-lda.pdf)
- [Attention is All You Need (Transformer)](https://arxiv.org/pdf/1706.03762.pdf)
- [Algorithms for Non-negative Matrix Factorization](https://proceedings.neurips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
- [T5: Exploring the Limits of Transfer Learning with a Unified
Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)
- [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
