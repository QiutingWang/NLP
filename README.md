# NLP
Natural Language Processing techniques give machines ability to ready, understand, and derive meaning from human languages.

Applications
--------------- 

Machine Translation, Speech Recognition, Sentiment Analysis, Question Answering, Automatic Summarization, Chatbots, Text Classification, Character Recognition, Spell Checking

Topic Modeling
--------------- 

- Useful Model
  - Latent Dirichlet Allocation (LDA)
     - Latent Dirichlet Allocation with Non-negative Matrix Factorization (LDA&NMF)
     - Hierarchical Latent Dirichlet Allocation (hLDA)
  - Latent Semantic Analysis (LSA)
  - Probabilistic Latent Semantic Analysis (PLSA)
  - Correlated Topic Model (CTM)
  - Author Topic Model
  - BERTopic


- Tools
  - Visualization: 
     - [pyLDAvis](https://github.com/bmabey/pyLDAvis/tree/master/notebooks)
     - [BertViz](https://github.com/jessevig/bertviz)
     - [exBert](https://huggingface.co/exbert/?model=bert-base-cased&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=0&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=true)
  - Quick LDA Topic Modeling:<https://lettier.com/projects/lda-topic-modeling/>


LLM Base-Model
---------------
- LLaMA
- GLM
- BLOOM
- GPT
- PaLM 

Fine-Tuning Technique
---------------------

- LoRA
- Prefix Tuning
- P-Tuning
- AdaLoRA
- QLoRA
- Adapter
- MaM
- RLHF

Paper
--------------- 
- [Latent Dirichlet Allocation](https://ai.stanford.edu/~ang/papers/jair03-lda.pdf)
- [Attention is All You Need (Transformer)](https://arxiv.org/pdf/1706.03762.pdf)
- [Algorithms for Non-negative Matrix Factorization](https://proceedings.neurips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
- [BERTopic: Neural topic modeling with a class-based TF-IDF procedure](https://arxiv.org/pdf/2203.05794.pdf)
- [T5: Exploring the Limits of Transfer Learning with a Unified
Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)
- [GPT-2: Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
- [Prompt Learning: Pre-train, Prompt, and Predict: A Systematic Survey of
Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586.pdf)
- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)
- [UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/pdf/1905.03197.pdf)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311v5.pdf)
- [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647.pdf)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)
- [RLHF: Deep Reinforcement Learning from Human Preferences](https://arxiv.org/pdf/1706.03741.pdf)
- [P-Tuning V1: GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)
- [P-Tuning V2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)
- [Prefix: Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)
- [Adapter: Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)
- [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013.pdf)
- [Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark](https://arxiv.org/pdf/2109.14545.pdf)
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073.pdf)
- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)
- [Proximal Policy Optimization Algorithms(PPO)](https://arxiv.org/pdf/1707.06347.pdf)
- [Knowledge Distillation: A Survey](https://link.springer.com/article/10.1007/s11263-021-01453-z)
- [A Survey on Model Compression for Large Language Models](https://arxiv.org/pdf/2308.07633.pdf)
- [DistillBert](https://arxiv.org/pdf/1910.01108.pdf)
- [TinyBert: Distilling BERT for Natural Language Understanding](https://aclanthology.org/2020.findings-emnlp.372/)
- [RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf)
- [ReAct](https://arxiv.org/pdf/2210.03629.pdf)
- [CoT: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)
