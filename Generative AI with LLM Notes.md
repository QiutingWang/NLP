# Generative AI with LLM Week1

- transformer
- self-attention
- vision transformer(ViT)
- collection of fundamental basic models:
  - BLOOM
  - GPT
  - LLaMa
  - Bert
  - PlaM
  - FLAN-T5
- Prompts and Completion:
  - Prompt(human-written instruction) with context window is given to LLM to complete the task with generated text.
- Application: text summarization, entity extraction, translation
- text generation before Transformer
  - RNNs, CNNs
  - challenges: language is complex. One word has multiple meanings ‰∏ÄËØçÂ§ö‰πâ„ÄÅÊåá‰ª£‰∏çÊòé
- üåüTransformer Architecture:
  - input as *tokens* passed into embedding layers, then put into encoder & decoder
    - tokenization:
      - <u>token IDs</u> matching with complete *words*
      - input token is processed <u>in parallel</u>
    - embedding layer:
      - trainable vector embedding space, high dimensional space
      - each token is represented as vector occupying an unique location
      - tokenID is matched with a <u>multi-dimensional vector</u>
      - in original paper, the size of vector=512
    - positional encoding
      - preserve the <u>word order position information</u>
  - self-attention
    - Role:
      - scan the relationship between <u>one world to all other words</u> with different attention weights
      - looks for high correlation between words, approving models' ability to encode language
    - Composition:
      - Encoder
      - Decoder
      - multi-headed self-attention in Transformer placed *in parallel*
        - the numbers of heads included in attention layer varied from *12~100* in different models
        - the weights of each head are <u>randomly initialized</u>, given enough time and data to **learn different aspects of language**
      - the output is processed through a <u>fully-connected feed-forward network</u>
        - the output is a logits proportion vector to the probability score for each token in dictionary.
      - use *residual connections and layer normalization* to prevent overfitting
    - softmax layer: <u>normalized a probability score</u> for each word
      - one single token which is the <u>most likely predicted token</u> has *highest score* among the rest
      - passing the output token <u>back to the input of decoder</u> to generate next token with highest score
- Text generation with Transformers:
  - scale efficiency, parallel process, attention to input's meaning
- LLM Classification by Architecture:
  - Encoder only model:
    - the output sequence and input sequence have the same length
    - use less commonly, usually used for classification tasks: sentiment analysis, word classification, named entity recognition
    - BERT, RoBerta
  - Encoder-Decoder model:
    - perform well on Seq2Seq task, such as translation, generation task, text summarization, question answering
    - input and output sequence can be *different length*
    - BART, T5
  - Decoder only model:
    - most commonly used today, can be used almost any tasks
    - usually used for text generation 
    - BLOOM, LLaMa, Jurassic, GPT
- Prompt and prompt engineering
  - prompt engineering: we need to *revise the prompts* several times to improve model performance.
    - **in-context learning**(ICL): <u>include example</u> of task inside the prompt
      - zero-shot: LLM with relative *smaller size* struggle in this prompt technique, improved by one-sot or few-shot.
      - one-shot: give one example of whole completed task
      - few-shot
    - context window almost has <u>input size limitation</u>(few thousand words)
    - if the model still cannot perform well even with few-shot prompting, we should *fine-tune* the model, which uses new data to perform additional training.
    - Scale Issues:
      - model with more parameters is capable to capture more understanding of language. Larger models are good at zero-shot prompting.
      - small LLM are only good at performing small number of tasks.
- Generative Configuration
  - `max new tokens`: maximum number of tokens of output the model generate
    - the output is operated with `greedy sampling`, that is, the token with <u>highest probability</u> is selected.
      - this method works well for *short generation*
    - another way called `random(weighted) sampling`, choosing an output word randomly using probability distribution to weight selection.
      - it reduces the likelihood for <u>choosing repeat words</u>.
  - `sample top K`: after using random weighted using probability, select an output from topK results.
  - `sample top P`: select an output using random-weighted method with <u>top-rank consecutive</u> results with cumulative probability $<p$
  - `temperature`:
    - softmax function parameter, affecting shape of probability distribution
    - higher the temperature($>1$), higher the randomness, less-likely tokens show up, *flatter or broader* distribution
    - lower temperature($< 1$), have a <u>strongly peaked probability distribution</u>, which is *concentrated* in a small number of words.
    - if we set temperature$=1$, we leave the softmax as *default*, <u>unchanged</u> probability distribution will be used.
- Generative AI Project Lifecycle
  - Scope: main tasks need to achieve
  - Select: choose an already existed model or pre-trained on my own
  - Adapt and Align: prompt engineering, fine-tuning, align with human feedback, evaluate
  - Application integration: optimize and deployment
- Pre-train LLM
  - importance of pre-train
    - primarily the input data almost unstructured textual data, from different sources
    - the model initializes <u>the structure or pattern</u> present in language, to complete training objective
    - we need to process the raw data to <u>improve data quality, address bias, and remove harmful content</u>.
      - Therefore, only *1~3% of original tokens* are used in pre-training.
    - during pre-training, model tries to minimize loss of objective, encoder generates embedding representation for each token
    - it requires lots of GPU
  - Autoencoding model(Encoder-only model):
    - pre-trained with *Masked Language Modeling(MLM)*
      - tokens of input sequence are randomly masked
      - training objective: predict the masked token to reconstruct text(`denoising`)
    - use bidirectional representation of input sequence
      - model understands *full context* of a token, not the previous words only.
  - Autoregressive model(decoder-only model):
    - pre-trained with *Causal Language Modeling(CLM)*
      - training objective: predict next token based on the previous series of tokens, also called `full language modeling`
      - the model has no knowledge of the end of the sentenceÁü•Ââç‰∏çÁü•Â∞æÔºå*unidirectional*.
      - it builds up a statistical representation of language
  - Sequence to Sequence Model(encoder-decoder):
    - Span corruption:
      - mask *random sequences* of input tokens
      - replaced the masked tokens by the unique <u>sentinel token</u>($x$)
        - sentinel token: added to vocabulary, but don't correspond to any actual word with input text.
      - decoder objective: different from model to model
        - For T5: reconstruct the mask token sequence autoregressively
      - the sentinel token is followed by predicted tokens
- Computational Challenges of Training LLM: running out of memory
  - Assume we use GPU RAM to train 1B-parameters:
    - 1 parameter=4 bytes(32-bit float)
    - 2 adam optimizer+8 bytes per parameter
    - Gradients+4 bytes per parameter
    - activation by temporary memory+8 bytes per parameter
    - in total, we need 80GB memory to train model
  - Solution to reduce memory required:
    - **Quantization**: 32-bit float(`FP32`)‚Üí16-bit float(`FP16`) | 8-bit integer(`INT8`) a <u>lower precision</u> space with *scaling factors*
      - range of FP32: $(3e^{-38},3e^{38})$, by default, model parameters are stored using this form.
        - sign: 1 bit; exponent: 8 bits; fraction: 23 bits
        - requires 4 bytes memory
      - FP16: range $(-65504, 65504)$
        - sign: 1 bit; exponent: 5 bits; fraction: 10 bits
        - requires 2 bytes memory
        - in total, we need 40GB memory to train model
        - drawback: it doesn't fit for integer calculation
      - optimize FP16 called `BFLOAT16`, also called `truncated FP32`
        - it carries full dynamic range of FP32, but only uses 16-bits
        - a good choice for deep learning models training.
        - requires 2 bytes memory
      - INT8:
        - range to represent number $\in (-128, 127)$
        - sign: 1 bit; fraction: 7 bits
        - requires 1 bytes memory to store 1 value
        - in total, we need 20GB memory to train model
    - if we want to run model(originally owns 1B parameters) on *single* GPU, plz <u>use 16-bit or 8-bit quantization</u>. 80GB is maximum memory of NVIDIA A100 GPU.
    - Hence, in most cases, we need to separate model across *multiple GPUs* to train in parallel.
- Efficient Multi-GPU Computation Strategies: Distributed Compute
  - **Distributed Data Parallel(DDP)**
    - distribute different branches to data into different GPU in parallel
    - copy models(parameter, gradient, optimizer) to each GPU
      - if model is too large, model copy is difficult to realize, solution: `model sharding`Ê®°ÂûãÂàÜÁâá
        - Application: *Fully Sharded Data Parallel(FSDP)*
          - PyTorch Tutorial: <https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/>
          - HuggingFace:<https://huggingface.co/docs/accelerate/usage_guides/fsdp>
          - *ZeRO*: subset of parameters of each GPU across GPUs
            - 3 stages: 
              1. $P_{optimization}$: reduce memory footprint by 4 times
              2. $P_{optimization+gradient}$: reduce memory footprint by 8 times
              3. $P_{optimization+gradient+parameters}$
          - it requires to <u>collect weights</u> before forward pass from all GPUs, and collect weights again after forward before backward pass. Finally, it is unsharded data.
          - support *offloading to GPU* is needed, to further reduce GPU memory utilization.
          - trade off performance and GPU utilization by `sharding factor` sets specific number of GPUs, to <u>configure sharding level</u>
            - more GPUs, communication volume between GPUs‚Üë, slowing down the computation
            - levels can be: fully replication, fully sharding, hybrid sharding
          - we can use FSDP for <u>both small and large size</u> of LLM, the performance is better than simple DDP.
    - synchronization gradients combine the results, then updates the model on each GPU
- Scaling Laws and Compute-Optimal Models
  - goal of pre-training: maximize model performance, minimize loss when predicting tokens
  - scaling choice:
    - dataset size, number of tokens‚Üë, performance‚Üë
    - model size, number of parameters‚Üë,performance‚Üë
  - constraint: computation budget: GPU hardware, training time(project timeline), financial budget
  - compute and quantifies required resources:
    - 1 petaflop/s-day:
      - number of floating operations performed at rate of 1 petaFLOP per second one day
      - 1 quarillion floating point operations per second
      - equals to <u>8 NVDA V100(or 2 NVDA A100 GPU)</u> chips running at full efficiency at 24 hrs
  - Chinchilla compute optimal models:
    - intuition: find an optimal number of parameters and training data for a given compute budget
    - conclusions:
      - very large model may be <u>over-parameterized and under-trained</u>
      - *smaller model* trained with *more data* can perform as well as larger models.
      - compute optimal training dataset size=**20X** number of parameters
      - Hence, sometime larger LLM(scaling effect) may not be a better choice.(eg: 50B Bloomberg GPT works well, but without enough training data in financial area)
- Pre-train for Domain Adaptation
  - in some cases, pre-train our own model from scratch is necessary for a better performance, when targeting domain with <u>uncommonly used vocabulary or language and in an idiosyncratic way</u>.
    - eg: medical, legal area, abbreviation or shorthand